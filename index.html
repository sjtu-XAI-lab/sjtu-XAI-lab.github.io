<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://getbootstrap.com/docs/5.0/examples/sidebars/sidebars.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="style.css">
    <title>Quanshi Zhang | 张拳石</title>
    <link rel="icon" type="image/png" href="img/sjtulogored.png">
</head>

<body>
    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg bg-dark bg-gradient navbar-dark navbar-fixed-top">
        <div class="container">
            <a href="#" class="navbar-brand px-12">Quanshi Zhang 张拳石 | SJTU interpretable ML lab</a>
            <button class="navbar-toggler justify-content-end" type="button" data-bs-toggle="collapse"
                data-bs-target="#navmenu">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse justify-content-end" id="navmenu">
                <ul class="navbar-nav ms-auto">
                    <div class="nav-item">
                        <a href="#people" class="nav-link">LAB MEMBER</a>
                    </div>
                    <div class="nav-item">
                        <a href="#about" class="nav-link">ABOUT</a>
                    </div>
                    <!-- <div class="nav-item">
                        <a href="#news" class="nav-link">NEWS</a>
                    </div> -->
                    <div class="nav-item">
                        <a href="#research" class="nav-link">RESEARCH MAP</a>
                    </div>
                    <div class="nav-item">
                        <a href="#publications" class="nav-link">PUBLICATION</a>
                    </div>
                    <div class="nav-item">
                        <a href="#post" class="nav-link">RESEARCH POST</a>
                    </div>
                    <div class="nav-item">
                        <a href="#contact" class="nav-link">CONTACT</a>
                    </div>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Shoucase -->
    <section id="intro" class="bg-dark bg-gradient text-light p-4 text-center text-sm-start">
        <div class="container">
            <div class="row">
                <div class="col-md-12 px-5 pt-5">
                    <font face="Verdana">
                        <h1>
                            Lab for Interpretability and Theory-Driven Deep Learning
                        </h1>
                        <!-- <h3> Our mission is to significantly improve people's lives through
                            our work in Artificial Intelligence
                        </h3> -->
                    </font>
                </div>
            </div>
        </div>
    </section>

    <!-- People -->
    <section id="people" class="bg-white pb-5 m-0">
        <div class="container">
            <h2 class="text-center text-dark mb-5"> Lab Member</h2>
            <!-- <h3 class="text-dark mb-5"> Team</h3> -->
            <div class="container pb-3 h-100">
                <div class="row d-flex justify-content-center align-items-center h-100">
                    <div class="col-10">
                        <div class="card border-card">
                            <div class="card-body p-4 bg-white">
                                <div class="d-flex text-black">
                                    <div class="flex-shrink-0">
                                        <img src="img/zqs.png" alt="Generic placeholder image"
                                            class="rounded-circle p-3" style="width: 210px;" />
                                        <div class="text-center">
                                            <span>
                                                <a href="http://qszhang.com/"
                                                    class="btn btn-default flex-grow-1">Homepage</a>
                                                <a href="mailto: zqs1022@sjtu.edu.cn"
                                                    class="btn btn-default flex-grow-1">Email</a>
                                            </span>
                                        </div>
                                    </div>
                                    <div class="flex-grow-1 ms-3">
                                        <h4 class="mb-1 text-center"><strong>Quanshi Zhang</strong></h4>
                                        <p class="text-muted p-0 text-center "
                                            style="text-align:justify; text-justify:inter-ideograph">Principal
                                            Investigator</p>
                                        <p class="mb-2 px-5" style="color: #2b2a2a;">
                                            Currently, I am an associate professor at <a
                                                href="https://en.sjtu.edu.cn/">Shanghai Jiaotong University</a>, leading
                                            the <strong>Lab for Interpretability and Theory-Driven Deep
                                                Learning</strong>.
                                            Before that, I received the B.S. degree in machine intelligence at <a
                                                href="https://english.pku.edu.cn/">Peking University</a>, China, in
                                            2009. I obtained the M.Eng. degree and the
                                            Ph.D. degree at <a href="https://www.u-tokyo.ac.jp/en/">University of
                                                Tokyo</a>, in 2011 and 2014, respectively, under
                                            the supervision of Prof. Ryosuke Shibasaki. In 2014, I became a postdoctoral
                                            associate at the <a href="https://www.ucla.edu/">University of California,
                                                Los Angeles</a>, under the
                                            supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun
                                                Zhu</a>.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row pb-2">
                <div class="col-lg-2"></div>
                <div class="col-lg-2"></div>
                <div class="col-lg-2">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/huiqideng.png" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Huiqi Deng</strong></h5>
                        <p class="text-muted mb-0"><small>Postdoc</small></p>
                        <span>
                            <a href="https://huiqideng1.netlify.app/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: hqdengsysu@163.com" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div class="col-lg-2">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/wenshen.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Wen Shen</strong></h5>
                        <p class="text-muted mb-0"><small>Postdoc</small></p>
                        <span>
                            <a href="https://ada-shen.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: wen_shen@tongji.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div class="col-lg-2"></div>
                <div class="col-lg-2"></div>
            </div>
            <div class="row py-2" style="margin-left:10%; margin-right:10%;">
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/xucheng.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Xu Cheng</strong></h5>
                        <p class="text-muted mb-0"><small>Phd</small></p>
                        <span>
                            <a href="https://cx1208.github.io/ChengXuSJTU.github.io/"
                                class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: xcheng8@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/xinwang.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Xin Wang</strong></h5>
                        <p class="text-muted mb-0"><small>Phd</small></p>
                        <span>
                            <a href="https://xinwang98.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: xin.wang@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/huilinzhou.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Huilin Zhou</strong></h5>
                        <p class="text-muted mb-0"><small>Phd</small></p>
                        <span>
                            <a href="https://zhouhuilin116.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: zhouhuilin116@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/jieren.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Jie Ren</strong></h5>
                        <p class="text-muted mb-0"><small>Phd</small></p>
                        <span>
                            <a href="https://jie-ren.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: ariesrj@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/diezhang.png" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Die Zhang</strong></h5>
                        <p class="text-muted mb-0"><small>Master</small></p>
                        <span>
                            <a href="https://zdgithub.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: zizhan52@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
            </div>
            <div class="row py-2" style="margin-left:10%; margin-right:10%;">
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/haozhang.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Hao Zhang</strong></h5>
                        <p class="text-muted mb-0"><small>Master</small></p>
                        <span>
                            <a href="https://haozhang37.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: 1603023-zh@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>

                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/mingjieli.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Mingjie Li</strong></h5>
                        <p class="text-muted mb-0"><small>Master</small></p>
                        <span>
                            <a href="http://mingjie.site/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: limingjie0608@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/zhanpengzhou.jpg" class="rounded-circle mb-0" style="height:100px;"
                            alt="Avatar" />
                        <h5 class="mb-0"><strong>Zhanpeng Zhou</strong></h5>
                        <p class="text-muted mb-0"><small>Undergraduate</small></p>
                        <span>
                            <a href="http://zzp1012.top/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: zzp1012@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/qihanren.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Qihan Ren</strong></h5>
                        <p class="text-muted mb-0"><small>Undergraduate</small></p>
                        <span>
                            <a href="https://nebularaid2000.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: renqihan@sjtu.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
                <div style="width:20%;">
                    <div class="p-0" style="text-align: center; vertical-align: middle;">
                        <img src="img/shaobowang.jpg" class="rounded-circle mb-0" style="height:100px;" alt="Avatar" />
                        <h5 class="mb-0"><strong>Shaobo Wang</strong></h5>
                        <p class="text-muted mb-0"><small>Undergraduate</small></p>
                        <span>
                            <a href="https://gszfwsb.github.io/" class="btn btn-default btn-sm">Homepage</a>
                            <a href="mailto: 181110315@stu.hit.edu.cn" class="btn btn-default btn-sm">Email</a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- About Our Lab -->
    <section id="about" class="bg-white text-dark" style="font-family:HelveticaNeue-Light, Helvetica Neue Light, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 80%;">
        <div class="container margin-right: auto;margin-left: auto; margin-top: 10px;">
            <div class="h2 text-center pb-3">About Our Lab
            </div>
            <p class="px-5" style="text-align:justify; text-justify:inter-ideograph">
                Due to the black-box nature of DNNs, interpretable machine learning has become a
                compelling topic. However, the trustworthiness and applicability of interpretable ML has
                been widely questioned in recent years. Core critisms include the following five aspects.
            <div class="px-5" style="text-align:justify; text-justify:inter-ideograph">
                <ul>
                    <li>Current explanation methods are built upon different heuristics, lacking common theoretical
                        foundations.</li>
                    <li>It is infeasible to objectively evaluate the correctness of the explanation results, in the
                        absence of ground truth explanations.</li>
                    <li>There is still a large gap between the semantic explanation result (e.g., the attribution-based
                        explanation) and the explanation of a DNN’s performance (e.g., the generalization power), which
                        cannot be unified.</li>
                    <li>Traditional semantic explanations cannot be used as feedbacks to guide the designing of DNNs,
                        thereby boosting DNN performances.</li>
                    <li>Most current methods boosting DNN performances are heuristic, and the theoretical mechanisms
                        behind their success are largely missing.</li>
                </ul>
            </div>

            </p>
            <p class="px-5" style="text-align:justify; text-justify:inter-ideograph">
                Therefore, our team aims to solve two key scientific problems:
            <div class="px-5" style="text-align:justify; text-justify:inter-ideograph">
                <ol>
                    <li>how to develop a theoretical system to unify different explanations of the semantics encoded in
                        a DNN (e.g., attribution-based explanations, taxonomy of semantic concepts, and etc) and unify
                        different explanations of the DNN performance (e.g., the generalization power, adversarial
                        robustness, adversarial
                        transferability, and etc). Besides, it is highly desirable to bridge the two types of
                        explanations. We believe that, an explanation system can be considered as trustworthy if the
                        methods in the system can be mutually verified.</li>
                    <li>how to extract the common mechanism behind different heuristic methods, which
                        include both explanation methods and current popular methods boosting the DNN performance.</li>
                </ol>
            </div>
            </p>
    </section>
    <!-- News -->
    <!-- <section id="news" class="bg-white text-dark p-5" style="font-family:HelveticaNeue-Light, Helvetica Neue Light, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;">
        <div class="container">
            <div class="h2 text-center pb-3">News
            </div>
            <p class="text-sm-start px-5">
                <span class="label label-success">2020.04.05</span>
                Update website. Check the preview version!!!
            </p>
            <p class="text-sm-start px-5">
                <span class="label label-success">2020.04.03</span>
                Initialize website.
            </p>
    </section> -->

    <!-- Research Directions -->
    <section id="research" class="p-5 bg-white text-center">
        <div class="h2 text-center">
            Research Map
        </div>
        <container class="p-5">
            <div class="col-lg-12" align="center">
                <div id="myDiagramDiv" style="width:85%; height: 750px;"></div>
            </div>
            <p></p>
        </container>

    </section>

    <!-- Publications -->
    <section id="publications" class="p-5 bg-white">
        <div class="h2 text-center">
            Selected Publications
        </div>
        <!-- chapter-7 -->
        <div class="container" style="margin-top: 30px;">
            <div class="row">
                <div class="col-xs-12 col-lg-4 img_responsive"
                    style="text-align: center; vertical-align: middle; margin-top:-15px;">
                    <img src="/blog-post/img/chapter-7/schematic.jpg" width="90%" />
                </div>

                <div class="col-xs-12 col-lg-8" style="text-align: left; vertical-align: middle;">
                    <strong>A Unified Approach to Interpreting and Boosting Adversarial Transferability
                    </strong>
                    <br>
                    <a href="https://xinwang98.github.io/">Xin Wang*</a>,
                    <a href="https://jie-ren.github.io/">Jie Ren*</a>,
                    Shuyun Lin, Xiangming Zhu,
                    <a href="https://yisenwang.github.io/">Yisen Wang</a>, and <strong>Quanshi Zhang</strong><br>
                    <div style="color: brown;">ICLR 2021</div>
                    <!-- <i> "A Unified Approach to Interpreting and Boosting Adversarial Transferability!" </i><br> -->
                    <a href="https://arxiv.org/abs/2010.04055" class="btn btn-default btn-sm">Paper</a>
                    <a href="https://github.com/xherdan76/A-Unified-Approach-to-Interpreting-and-Boosting-Adversarial-Transferability"
                        class="btn btn-default btn-sm">Code</a>
                    <a href="#c7" class="btn btn-default btn-sm">Post</a>
                    <a href="https://zhuanlan.zhihu.com/p/369883667" class="btn btn-default btn-sm">Blog (Chinese)</a>
                </div>
            </div>
        </div>
    </section>


    <!-- Blog -->
    <section id="post" class="p-5 bg-white" style="font-family:HelveticaNeue-Light, Helvetica Neue Light, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;">
        <div class="h2 text-center">Research Post</div>
        <div
            style="margin-right: auto;margin-left: auto; margin-top: 10px; margin-bottom: 100px; width: 70%;text-align:justify; text-justify:inter-ideograph">
            <div style="text-align: center;">
                <h3 id="Interaction">Interaction Interpretability of Neural Networks</h3>
            </div>
            <p id="c1"><br><br></p>
            <h4><strong> Chapter 1 Introduction</strong> </h4>
            <p>
                In 2018, I started to post on the online platform <a
                    href=" https://zhuanlan.zhihu.com/p/264871522/">"Zhihu"</a> (similar
                to
                Quora) for my research about
                the interpretability of DNNs. At that point my paper was finally accepted for the first time after being
                repeatedly discouraged by the trend of pursuing higher prediction scores on a certain task. Only then
                can I
                say it out loud “beyond visualization, there is another path to interpretability.” Two years later, I
                still
                insist on posting, because I want everyone to know if all explanation methods are only self-justified,
                but
                cannot be verified by each other, then neither of them can be considered as a fully correct method.
                There
                are only a few solid studies in Explainable AI, such as the Shapley value, which satisfies four
                mathematical
                axioms for a "correct attribution heatmap", including linearity, nullity, symmetry, and efficiency.
                Explanations that satisfy the above four axioms can be considered as rigorous explanations. If there is
                no
                theory to ensure the rigor and objectivity of explanation methods, then sooner or later the
                interpretation
                research will disappear.
                (Note that this is not an article to discourage you from doing research on Explainable AI. I am
                semi-confident in the future of explainable AI —— there is a chance that nothing you will achieve, that
                you
                will end up with publishing some papers, or that you will actually do some solid work. This is sometimes
                the
                most exciting and promising research. Therefore, I still keep going.)
            </p>
            <p>The "objective rigor" of interpretability often means "generality" and "uniqueness", i.e., "the
                explanation
                of the only standard. "Generality" is easy to understand, which means that the algorithm should be more
                standard and have more connections to previous theories, rather than being a scenario-specific ad-hoc
                technique. In contrast, the "uniqueness" has been rarely mentioned, some people even ask "why should the
                explanation be unique?" Here we need to impose some restrictions on "what is a good explanation？" E.g.,
                what
                conditions must be satisfied by a good explanation, and then "uniqueness" is embodied in the unique
                solution
                under these conditions. To a smaller extent, conditions for explanations can be the four axioms
                corresponding to the Shapley value; to a larger extent, conditions for explanations can also be the
                scope of
                application of the interpretability metrics. For example, the same metric can "explain semantics,
                generalization, and transferability".
                <!-- <div class="p-5 text-center" style="text-align:center; width:100%">
                <iframe width="800" height="400" src="https://www.youtube.com/embed/DEvI5silI4Q"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
                <figcaption class="pt-3 px-3 card-img-bottom">
                    <h2 class="h5 font-weight-bold mb-2 font-italic">Video 1</h2>
                    <p class="mb-0 text-small text-muted font-italic">Lorem ipsum dolor sit amet, consectetur
                        adipisicing
                        elit, sed do eiusmod tempor.</p>
                </figcaption>
            </div> -->
            </p>
            <p>
                Let's jump out of the topic that we just discussed and now let's face the whole picture of explainable
                AI.
                Whether a research direction is "alive" or "sustainable in the future" does not lie in the number of
                papers
                published in the field, nor in the number of citations, but in the number of essential problems in the
                research direction that have not been addressed mathematically. (Here we refer to the solid modeling,
                rather
                than deliberately hooking up a new concept). After all, I still remember the saying by Professor
                Song-Chun
                Zhu "deep learning has died" in 2006, a sentence enough for me to digest for many years - when a person
                blocks all the shortcuts for publishing papers, you need to plan a road from the mud, which may be the
                right
                way to go - although it is likely to die on the halfway, we have to be prepared.<br>
            </p>
            <p>Now, let's go back to talk about the explainable AI. The original purpose of explainable AI is very
                simple,
                that is, to provide theoretical guidance for the training and design of neural networks, and to test the
                reliability of the information modeled by neural networks. In brief, a reliable explainable AI study
                needs
                to satisfy requirements simultaneously:
            <ol>
                <li>Direct theoretical modeling of the target to be explained is required, rather than proposing
                    indirect
                    algorithms intuitively.</li>
                <li>Quantitative explanatory results are needed, instead of qualitative ones.</li>
                <li>The objectivity (or rigor) of the explanatory results needs to be evaluated, rather than just
                    requiring
                    that the explanatory results look good.</li>
                <li>For many emerging problems in explainable AI, sometimes we cannot provide a direct evaluation or the
                    ground-truth of neural networks. Thus, we need a more solid theoretical system to prove the rigor of
                    mathematical modeling.
                    <ul>
                        <li>For example, the rigor of Shapley value is guaranteed by the four axioms it satisfies.
                            Similarly, an explanation method needs to satisfy a large number of axioms or exhibits good
                            properties to demonstrate the rigor of the explanation method.</li>
                        <li>Besides, if different explanation methods or metrics can be mutually validated, then such
                            explanations are often more rigorous.</li>
                    </ul>
                </li>
                <li>Explanation theories need to be extended to explaining various phenomena in real-world applications,
                    or
                    to provide direct guidance on the design and training of neural networks.</li>
            </ol>
            </p>
            <p id="c2"><br><br></p>
            <h4><strong>Chapter 2</strong></h4>
            <p id="c3"><br><br></p>
            <h4><strong>Chapter 3</strong></h4>
            <p id="c4"><br><br></p>
            <h4><strong>Chapter 4</strong></h4>
            <p id="c5"><br><br></p>
            <h4><strong>Chapter 5</strong></h4>
            <p id="c6"><br><br></p>
            <h4><strong>Chapter 6</strong></h4>
            <p id="c7"><br><br></p>
            <h4><strong> Chapter 7 From Practice to Theory: Adversarial Transferability of Neural
                    Networks</strong> </h4>
            <h4><strong>Introduction</strong></h4>
            <p>
                Hello, we are Xin Wang and Jie Ren, students of Dr. Quanshi Zhang. In this article, we would like to
                share some new insights to adversarial transferability of DNNs.
            </p>
            <p>
                Reference: Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, Quanshi Zhang, <a
                    href="https://arxiv.org/abs/2010.04055">"A Unified
                    Approach to Interpreting and Boosting Adversarial Transferability"</a> in ICLR 2021
            </p>
            <p><i>—— Note that this paper is not based on empirical experiments, instead, we investigated the nature of
                    adversarial transferability of DNN (though not very rigorously), while the experiments serve as an
                    aid to verification. Please pay attention to this.</i></p>
            <p>
                Although deep learning has been rapidly developed in the last decades, they have encountered
                bottlenecks. On the one hand, many algorithms are proposed based on trial and error from empirical
                experiments, which lack solid and reliable theoretical foundations; on the other hand, these studies
                usually based on simple assumptions, which are detached from the actual problems and cannot guide the
                design and optimization of DNNs.
            </p>
            <p>
                Therefore, we believe the new trend for deep learning is to build a complete theoretical system and use
                it to guide applications. If an algorithm only stays at the experimental level without theoretical
                explanation, then such an algorithm just like “Shennong”, a divine farmer in ancient China, who tasted
                hundreds of herbs to test their medical value. People cannot explain why such an algorithm works, then
                the significance of the algorithm might be limited to make a small improvement on performance, which
                constrains further explorations. In contrast, if we can prove and grasp the common effective components
                shared by many different algorithms, just like deriving the common effective ingredients from hundreds
                of herbs, then the algorithm can be considered effective and reliable. Thus, these insights can be used
                to guide the design of network structure and improve the performance of DNNs.
            </p>
            <p>In this article, we take the adversarial transferability of DNNs as an example to made attempts along
                this direction. We have obtained some shallow conclusions to suggest possible explanations for the
                previous algorithms that were proposed to improve the adversarial transferability. We hope to extract
                some common effects in previous algorithms, and further improve the algorithm transferability. Although
                we cannot ensure that these conclusions essentially reflect the nature of adversarial transferability,
                we try to ensure that our results reflect some of the common effects of previous algorithms. Besides,
                taking this study as a basis, we further explore the intrinsic nature of adversarial transferability.
            </p>
            <h4><strong>Enhancing adversarial transferability: from practice to theory</strong></h4>
            <p>
                In recent years, adversarial samples have attracted much attention in the field of deep learning. Behind
                the superior performance of neural networks, there lies a very dangerous security risk —— adding a small
                imperceptible perturbation to the input sample can completely change the prediction of a neural network.
                Such maliciously modified input samples are called adversarial samples. Researchers also found that
                these adversarial samples are transferable, which means that an adversarial sample generated on neural
                network A may also be able to attack neural network B.
            </p>
            <p>
                Many approaches have been proposed to enhance the adversarial transferability. However, the intrinsic
                mechanism of these methods to enhance the transferability remains unclear, just like different herbs
                tasted by Shennong. Although these methods can indeed enhance transferability, people are still unclear
                about which components that really work. Here we summarize several common approaches to enhance
                transferability.
            <ul>
                <li><strong>Variance-Reduced Attack (VR Attack)</strong>: During the attack, Gaussian noise is added to
                    the input image to smooth the gradient against the input image.</li>
                <li><strong>Momentum Iterative Attack (MI Attack)</strong>: Integrate the gradient momentum in the
                    optimization of adversarial perturbations.</li>
                <li><strong>Skip Connection Method (SGM Attack)</strong>: For the residual block structure of the
                    network, increase the gradient weight of the skip-connection branch in the backpropagation process.
                </li>
                <li><strong>Diversity Input Attack (DI Attack)</strong>: During the attack, introduce random padding and
                    random resizing on the input image to increase the diversity.</li>
                <li><strong>Translation Invariant Attack (TI Attack)</strong>: Convolve the image gradient during the
                    attack.</li>
            </ul>
            Our goal is to extract the common effects from the above methods.
            </p>
            <h4><strong>Adversarial Transferability: Extraction of useful components</strong></h4>
            <p>We aim to explain adversarial transferability from a new perspective, the game-theoretic interaction
                inside the adversarial perturbation. This explanation further extracts the common effects of the
                previous transferability-boosting approaches.</p>
            <h4><strong>Basis: Game-theoretic interaction</strong></h4>
            <p>We define the game-theoretic interaction between two units of the adversarial perturbation based on the
                Shapley value in game theory. Shapley values measure the importance of different adversarial
                perturbation units. For two adversarial perturbation units i and j, the game-theoretic interaction
                between them embodies the influence of i and j on each other, which can be defined as: the importance of
                i when j is always present minus the importance of i when j is not present. If the interaction is
                greater than 0, it means that there is a positive interaction between i and j, which facilitates the
                importance of each other. If the interaction is less than 0, it means that there is a negative
                interaction between i and j, which decreases the importance of each other.</p>
            <h4><strong>Motivation: multi-step attack vs. single-step attack</strong></h4>
            <p>Previous studies have suggested that, compared with the single-step attacks, the adversarial
                perturbations generated by multi-step attacks are more likely to overfit to the source neural network,
                which result in lower transferability. We analyzed multi-step attacks and single-step attacks from the
                perspective of game-theoretic interactions, and deduced that the adversarial perturbations obtained from
                multi-step adversarial attacks usually exhibit larger game-theoretic interactions than those obtained
                from single-step attacks.</p>
            <h4><strong>Hypothesis: there is a negative correlation between adversarial transferability and
                    game-theoretic interaction</strong></h4>
            <p>For single-step and multi-step attacks, we observed that:
            <ul>
                <li>Adversarial transferability: multi-step attack < single-step attack</li>
                <li>Game-theoretic interaction: multi-step attack > single-step attack</li>
            </ul>
            <p>
                Based on the above observations, we consider that the complex game-theoretic interactions reveal the
                overfitting of the adversarial perturbations towards the source model, thus compromising its
                transferability to the target network. Thus, we propose the following hypothesis.<br>
                <i>The transferability of the adversarial perturbation is negatively correlated with the interaction
                    inside the perturbation.
                </i>
            </p>
            <h4><strong>Verification: Negative correlation between adversarial transferability and game-theoretic
                    interaction</strong></h4>
            <p>We empirically compare the interactions of the less transferable adversarial perturbations with those of
                the more transferable ones, and thus the negative correlation is verified. Based on the ImageNet
                dataset, we generated adversarial perturbations on ResNet-34/152 (RN-34/152) and DenseNet-121/201
                (DN-121/201), respectively, and transferred adversarial perturbations generated on each ResNet to
                DenseNets. Similarly, we also transferred adversarial perturbations generated on each DenseNet to
                ResNets. Figure 1 shows the negative correlation between adversarial transferability and game-theoretic
                interaction, where the horizontal axis indicates the average interaction of adversarial perturbations
                through all testing images, and the vertical axis represents the average transfer utility of adversarial
                perturbations on the target DNN.</p>
            <figure class="p-0 text-center">
                <img src="./blog-post/img/chapter-7/fig1.jpg" width="80%">
                <figcaption class="pt-3 px-5 card-img-bottom">
                    <p class="text-muted font-italic">
                        Figure 1. The negative correlation between the transfer utility and the interaction. The
                        correlation is
                        computed as the Pearson correlation. The blue shade in each subfigure represents the 95%
                        confidence interval of the linear regression.</p>
                </figcaption>
            </figure>
            <h4><strong>A unified explanation: Reducing game-theoretic interaction is the common effects of various
                    transferability-boosting methods</strong></h4>
            <p>Beginning from the negative correlation between adversarial transferability and game-theoretic
                interaction, we extracted the common effective component from various previous methods of enhancing
                adversarial transferability, which is reducing game-theoretic interactions inside the adversarial
                perturbation. We find that although the starting points and implementations of previous approaches to
                enhance adversarial transferability are different, they all share the same effects - they all reduce the
                game-theoretic interaction inside the adversarial perturbation during the attack. We theoretically
                demonstrate that VR Attack, MI Attack, and SGM Attack produce adversarial perturbations with lower
                interactions than the most common baseline (PGD Attack); we experimentally verify that DI Attack and TI
                Attack reduce the interaction between perturbations.</p>
            <figure class="p-0 text-center">
                <img src="./blog-post/img/chapter-7/fig2.jpg" width="80%">
                <!-- <figcaption class="pt-3 px-5 card-img-bottom">
                    <p class="text-muted font-italic">
                        Figure 2. The research.</p>
                </figcaption> -->
            </figure>
            <h4><strong>Verification: Effectiveness of reducing game interactions</strong></h4>
            <p>To further verify the negative correlation between the game-theoretic interaction and adversarial
                transferability, we propose a loss function based on the game-theoretic interaction - directly
                penalizing the interaction inside the perturbation in the attack. We optimize both the classification
                loss function and the interaction-based loss function to generate the adversarial perturbation, which is
                called the Interaction-Reduced Attack (IR Attack).</p>
            <!-- formula -->
            \begin{equation}
            \max _{\delta}\left[\ell(h(x+\delta), y)-\lambda \ell_{\text {interaction }}\right], \quad \ell_{\text
            {interaction }}=\mathbb{E}_{i, j}\left[I_{i j}(\delta)\right] \quad \text { s.t. }\|\delta\|_{p} \leq
            \epsilon, x+\delta \in[0,1]^{n}
            \end{equation}
            <p>Experimental results show that the interaction-based loss function can significantly improve the
                transferability of the adversarial perturbation. As shown in Table 1, compared with baseline methods,
                the interaction loss function improves the transferability of adversarial examples by 8.2%~35.4%. It is
                worth noting that the interaction loss is only a direct measure to reduce the game-theoretic
                interaction, and we can combine it with previous attacks that reduce the interaction inside the
                perturbation to jointly reduce the interaction inside the perturbations to further improve the
                adversarial transferability. Therefore, we propose HybridIR Attack (MI+VR+SGM+IR Attack). In our
                experiments, the HybridIR Attack increases the transferability of the adversarial examples from
                54.6%~98.8% to 70.2%~99.1%.</p>
            <figure class="p-0 text-center">
                <figcaption class="pt-3 px-5 card-img-bottom">
                    <p class="text-muted font-italic">
                        Table 1: The success rates of \( L_1 \) and \( L_\infty \) black-box attacks crafted on six
                        source models,
                        including AlexNet, VGG16, RN-34/152, DN-121/201, against seven target models. Transferability of
                        adversarial perturbations can be enhanced by penalizing interactions.</p>
                </figcaption>
                <img src="./blog-post/img/chapter-7/table1.png" width="80%">

            </figure>
            <figure class="p-0 text-center">
                <figcaption class="pt-3 px-5 card-img-bottom">
                    <p class="text-muted font-italic">
                        Table 2: The success rates of \( L_\infty \) black-box attacks crafted on the ensemble model
                        (RN-34+RN-
                        152+DN-121) against nine target models.</p>
                </figcaption>
                <img src="./blog-post/img/chapter-7/table2.png" width="80%">
            </figure>
        </div>

    </section>

    <!-- contact -->
    <section id="contact" class="bg-dark bg-gradient text-center text-white pb-5 px-5">
        <div class="footer bg-dark-gradient">
            <div class="container">

                <img src="http://en.sjtu.edu.cn/images/logo_white.png" alt=""
                    style="max-width:100%;max-height:50px;margin-bottom:0px;margin-top:10px;">
                <div class="row p-0" style="vertical-align:middle">
                    <div class="col-lg-3">
                        <h3>Address</h3>
                        <p> 800 Dongchuan RD. Minhang District, Shanghai, China
                            <br> 上海市闵行区东川路800号
                        </p>
                    </div>
                    <div class="col-lg-3">
                        <h3>Email </h3>
                        <p>zqs1022@sjtu.edu.cn</p>
                    </div>
                    <div class="col-lg-3">
                        <h3> Follow us</h3>
                        <p> media platform</p>
                        <div class="row">
                            <div class="media-list">
                                <p>
                                    <span>
                                        <a href="https://github.com/sjtu-XAI-lab"><i class="fab fa-github"
                                                style="font-size: 20px;"></i></a>
                                        <a href="https://twitter.com/QuanshiZ"> <i class="fab fa-twitter"
                                                style="font-size: 20px;"></i>
                                        </a>

                                        <a href="https://www.youtube.com/user/zqs1022/featured">
                                            <i class="fab fa-youtube" style="font-size: 20px;"></i>
                                        </a>
                                        <a href="https://www.baidu.com">
                                            <i class="fab fa-zhihu big-icon" style="font-size: 20px;"></i>
                                        </a>
                                        <a href="https://www.baidu.com">
                                            <i class="fab fa-weixin big-icon" style="font-size: 20px;"></i>
                                        </a>
                                    </span>
                                </p>

                            </div>

                        </div>
                    </div>
                    <div class="col-lg-3">
                        <center>
                            <div align="center" style="width:30%">
                                <script type="text/javascript" id="clstr_globe"
                                    src="//clustrmaps.com/globe.js?d=grr8tSJTdsbdU-vHO6Of-5W7jpLTZvSVYTu6BUgf02M"></script>
                            </div>
                        </center>
                        This page has been visited for
                        <a href="https://www.easycounter.com/">
                            <img src="https://www.easycounter.com/counter.php?gszfwsb" border="0"
                                alt="HTML Hit Counter"></a> times
                    </div>
                </div>
            </div>
        </div>
        <button onclick="topFunction()" id="myBtn" title="Go to top" class="bg-dark bg-gradient">Top</button>
        <style>
            #myBtn {
                display: none;
                position: fixed;
                bottom: 60px;
                right: 30px;
                z-index: 99;
                border: none;
                outline: none;
                color: white;
                cursor: pointer;
                height: 50px;
                width: 50px;
                border-radius: 50%;
                font-weight: bold;
            }

            #myBtn:hover {
                background-color: #000;
            }
        </style>
    </section>

    <footer class="bg-dark bg-gradient text-center text-white fixed-bottom p-0">
        <p class="text-white">Copyright &copy; 2022. Lab for Interpretability and Theory-Driven Deep Learning. All
            rights reserved.</p>
    </footer>


    <script text="text/javascript">
        window.onscroll = function () { scrollFunction() };
        function scrollFunction() {
            if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
                document.getElementById("myBtn").style.display = "block";
            } else {
                document.getElementById("myBtn").style.display = "none";
            }
        }
        function topFunction() {
            document.body.scrollTop = 0;
            document.documentElement.scrollTop = 0;
        }
    </script>


    <!-- JavaScript Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>
    <script src="go-debug.js"></script>
    <script src="HyperlinkText.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- <script text="text/javascript" src="test.js"></script> -->
    <script text="text/javascript" src="research-map.js"></script>
    <script type="text/javascript" id="clstr_globe"
        src="//clustrmaps.com/globe.js?d=ZFAQauiNfOzaVSFWRxGPxdiZ5f0oN235Jn4ihTE3hsw"></script>
    <script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
</body>

</html>